{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf2a881d-1752-4d82-92fc-c32249cb6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "from going_modular import data_setup, engine\n",
    "\n",
    "from typing import Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from going_modular.engine import train_step, test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdd714c-0abb-443c-a728-cab6ab0c50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "586c1e86-6941-42d2-9fb2-71183a93c29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pizza_steak_sushi directory exists.\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, creating one...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Download pizza, steak, sushi data\n",
    "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "        print(\"Downloading pizza, steak, sushi data...\")\n",
    "        f.write(request.content)\n",
    "\n",
    "    # Unzip pizza, steak, sushi data\n",
    "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "        print(\"Unzipping pizza, steak, sushi data...\") \n",
    "        zip_ref.extractall(image_path)\n",
    "\n",
    "    # Remove .zip file\n",
    "    os.remove(data_path / \"pizza_steak_sushi.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f182b8-9db6-4379-aa2a-6799765c63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "def set_seeds(seed: int=42):\n",
    "    \"\"\"Sets random sets for torch operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): Random seed to set. Defaults to 42.\n",
    "    \"\"\"\n",
    "    # Set the seed for general torch operations\n",
    "    torch.manual_seed(seed)\n",
    "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a80e9ab9-fe7d-4fb6-9485-182007a0419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db32a998-e370-4159-8fcd-b82675794b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.1+cu102'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b063eb4a-5f9f-482d-9c55-174f51601690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.efficientnet_b0(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8419bb20-cc3a-42f5-812b-593d83b37a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "automatic_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9552169-50ec-4dd9-8cf0-42b02fa17011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dirs\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir, test_dir=test_dir, transform=automatic_transforms, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd692280-9d47-474d-af6e-ecab2f6c5957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all base layers by setting requires_grad attribute to False\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Since we're creating a new layer with random weights (torch.nn.Linear), \n",
    "# let's set the seeds\n",
    "set_seeds() \n",
    "\n",
    "# Update the classifier head to suit our problem\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=len(class_names),bias=True).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68942500-7711-43de-9f7f-9459f56581ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name))                                      Output Shape         Param #\n",
       "====================================================================================================\n",
       "EfficientNet                                                 --                   --\n",
       "├─Sequential (features)                                      [32, 1280, 7, 7]     --\n",
       "│    └─ConvNormActivation (0)                                [32, 32, 112, 112]   --\n",
       "│    │    └─Conv2d (0)                                       [32, 32, 112, 112]   (864)\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   (64)\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   --\n",
       "│    └─Sequential (1)                                        [32, 16, 112, 112]   --\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   (1,448)\n",
       "│    └─Sequential (2)                                        [32, 24, 56, 56]     --\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     (6,004)\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     (10,710)\n",
       "│    └─Sequential (3)                                        [32, 40, 28, 28]     --\n",
       "│    │    └─MBConv (0)                                       [32, 40, 28, 28]     (15,350)\n",
       "│    │    └─MBConv (1)                                       [32, 40, 28, 28]     (31,290)\n",
       "│    └─Sequential (4)                                        [32, 80, 14, 14]     --\n",
       "│    │    └─MBConv (0)                                       [32, 80, 14, 14]     (37,130)\n",
       "│    │    └─MBConv (1)                                       [32, 80, 14, 14]     (102,900)\n",
       "│    │    └─MBConv (2)                                       [32, 80, 14, 14]     (102,900)\n",
       "│    └─Sequential (5)                                        [32, 112, 14, 14]    --\n",
       "│    │    └─MBConv (0)                                       [32, 112, 14, 14]    (126,004)\n",
       "│    │    └─MBConv (1)                                       [32, 112, 14, 14]    (208,572)\n",
       "│    │    └─MBConv (2)                                       [32, 112, 14, 14]    (208,572)\n",
       "│    └─Sequential (6)                                        [32, 192, 7, 7]      --\n",
       "│    │    └─MBConv (0)                                       [32, 192, 7, 7]      (262,492)\n",
       "│    │    └─MBConv (1)                                       [32, 192, 7, 7]      (587,952)\n",
       "│    │    └─MBConv (2)                                       [32, 192, 7, 7]      (587,952)\n",
       "│    │    └─MBConv (3)                                       [32, 192, 7, 7]      (587,952)\n",
       "│    └─Sequential (7)                                        [32, 320, 7, 7]      --\n",
       "│    │    └─MBConv (0)                                       [32, 320, 7, 7]      (717,232)\n",
       "│    └─ConvNormActivation (8)                                [32, 1280, 7, 7]     --\n",
       "│    │    └─Conv2d (0)                                       [32, 1280, 7, 7]     (409,600)\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     (2,560)\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 1, 1]     --\n",
       "├─Sequential (classifier)                                    [32, 3]              --\n",
       "│    └─Dropout (0)                                           [32, 1280]           --\n",
       "│    └─Linear (1)                                            [32, 3]              3,843\n",
       "====================================================================================================\n",
       "Total params: 4,011,391\n",
       "Trainable params: 3,843\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (G): 12.31\n",
       "====================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3452.09\n",
       "Params size (MB): 16.05\n",
       "Estimated Total Size (MB): 3487.41\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(32, 3, 224, 224), verbose=0, col_width=20, row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "210d6837-2e92-4c2f-bf42-61a601c6bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca90b40-c967-4e12-99f3-e39c09ee4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "541ad9d3-9da2-4694-8490-1b48b1a0e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model : torch.nn.Module,\n",
    "         train_dataloader : torch.utils.data.DataLoader,\n",
    "         test_dataloader : torch.utils.data.DataLoader,\n",
    "         optimizer : torch.optim.Optimizer,\n",
    "         loss_fn : torch.nn.Module,\n",
    "         epochs: int,\n",
    "         device : torch.device) -> Dict[str, List]:\n",
    "    \n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "      model: A PyTorch model to be trained and tested.\n",
    "      train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "      test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "      optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "      epochs: An integer indicating how many epochs to train for.\n",
    "      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "      \n",
    "    Returns:\n",
    "      A dictionary of training and testing loss as well as training and\n",
    "      testing accuracy metrics. Each metric has a value in a list for \n",
    "      each epoch.\n",
    "      In the form: {train_loss: [...],\n",
    "                train_acc: [...],\n",
    "                test_loss: [...],\n",
    "                test_acc: [...]} \n",
    "      For example if training for epochs=2: \n",
    "              {train_loss: [2.0616, 1.0537],\n",
    "                train_acc: [0.3945, 0.3945],\n",
    "                test_loss: [1.2641, 1.5706],\n",
    "                test_acc: [0.3400, 0.2973]} \n",
    "    \"\"\"\n",
    "    \n",
    "    # create empty results dict\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        test_loss, test_acc = test_step(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Print out what's happening\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "        \n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "#         ### New: Experiment tracking ###\n",
    "#         # Add loss results to SummaryWriter\n",
    "#         writer.add_scalars(main_tag=\"Loss\", \n",
    "#                            tag_scalar_dict={\"train_loss\": train_loss,\n",
    "#                                             \"test_loss\": test_loss},\n",
    "#                            global_step=epoch)\n",
    "        \n",
    "#         # Add accuracy results to SummaryWriter\n",
    "#         writer.add_scalars(main_tag=\"Accuracy\", \n",
    "#                            tag_scalar_dict={\"train_acc\": train_acc,\n",
    "#                                             \"test_acc\": test_acc}, \n",
    "#                            global_step=epoch)\n",
    "        \n",
    "#         # Track the PyTorch model architecture\n",
    "#         writer.add_graph(model=model, \n",
    "#                          # Pass in an example input\n",
    "#                          input_to_model=torch.randn(32, 3, 224, 224).to(device))\n",
    "        \n",
    "#     # Close the writer\n",
    "#     writer.close()\n",
    "    \n",
    "    ### End new ###\n",
    "    \n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6510d1ce-0081-4ae1-8b24-5fb4ed672bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c33f8d85fe04dbfb14543ae66b707e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0908 | train_acc: 0.4453 | test_loss: 0.9066 | test_acc: 0.5284\n",
      "Epoch: 2 | train_loss: 0.8667 | train_acc: 0.7773 | test_loss: 0.7938 | test_acc: 0.7538\n",
      "Epoch: 3 | train_loss: 0.7633 | train_acc: 0.7930 | test_loss: 0.7378 | test_acc: 0.7936\n",
      "Epoch: 4 | train_loss: 0.7271 | train_acc: 0.7383 | test_loss: 0.6497 | test_acc: 0.8759\n",
      "Epoch: 5 | train_loss: 0.6380 | train_acc: 0.7852 | test_loss: 0.6222 | test_acc: 0.9072\n",
      "Epoch: 6 | train_loss: 0.5963 | train_acc: 0.7617 | test_loss: 0.5554 | test_acc: 0.9072\n",
      "Epoch: 7 | train_loss: 0.5376 | train_acc: 0.9219 | test_loss: 0.5253 | test_acc: 0.9176\n",
      "Epoch: 8 | train_loss: 0.5546 | train_acc: 0.7930 | test_loss: 0.5231 | test_acc: 0.9176\n",
      "Epoch: 9 | train_loss: 0.5638 | train_acc: 0.8008 | test_loss: 0.4919 | test_acc: 0.9280\n",
      "Epoch: 10 | train_loss: 0.5136 | train_acc: 0.8086 | test_loss: 0.4180 | test_acc: 0.8655\n"
     ]
    }
   ],
   "source": [
    "# Set the random seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Setup training and save the results\n",
    "results = train(model=model,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=10,\n",
    "                       device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fac49b61-6291-4bfd-8cb0-cbad002f7f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%\n",
      "10/10 [00:21<00:00, 2.08s/it]\n",
      "Epoch: 1 | train_loss: 1.0908 | train_acc: 0.4453 | test_loss: 0.9066 | test_acc: 0.5284\n",
      "Epoch: 2 | train_loss: 0.8667 | train_acc: 0.7773 | test_loss: 0.7938 | test_acc: 0.7538\n",
      "Epoch: 3 | train_loss: 0.7633 | train_acc: 0.7930 | test_loss: 0.7378 | test_acc: 0.7936\n",
      "Epoch: 4 | train_loss: 0.7271 | train_acc: 0.7383 | test_loss: 0.6497 | test_acc: 0.8759\n",
      "Epoch: 5 | train_loss: 0.6380 | train_acc: 0.7852 | test_loss: 0.6222 | test_acc: 0.9072\n",
      "Epoch: 6 | train_loss: 0.5963 | train_acc: 0.7617 | test_loss: 0.5554 | test_acc: 0.9072\n",
      "Epoch: 7 | train_loss: 0.5376 | train_acc: 0.9219 | test_loss: 0.5253 | test_acc: 0.9176\n",
      "Epoch: 8 | train_loss: 0.5546 | train_acc: 0.7930 | test_loss: 0.5231 | test_acc: 0.9176\n",
      "Epoch: 9 | train_loss: 0.5638 | train_acc: 0.8008 | test_loss: 0.4919 | test_acc: 0.9280\n",
      "Epoch: 10 | train_loss: 0.5136 | train_acc: 0.8086 | test_loss: 0.4180 | test_acc: 0.8655\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "100%\n",
    "10/10 [00:21<00:00, 2.08s/it]\n",
    "Epoch: 1 | train_loss: 1.0908 | train_acc: 0.4453 | test_loss: 0.9066 | test_acc: 0.5284\n",
    "Epoch: 2 | train_loss: 0.8667 | train_acc: 0.7773 | test_loss: 0.7938 | test_acc: 0.7538\n",
    "Epoch: 3 | train_loss: 0.7633 | train_acc: 0.7930 | test_loss: 0.7378 | test_acc: 0.7936\n",
    "Epoch: 4 | train_loss: 0.7271 | train_acc: 0.7383 | test_loss: 0.6497 | test_acc: 0.8759\n",
    "Epoch: 5 | train_loss: 0.6380 | train_acc: 0.7852 | test_loss: 0.6222 | test_acc: 0.9072\n",
    "Epoch: 6 | train_loss: 0.5963 | train_acc: 0.7617 | test_loss: 0.5554 | test_acc: 0.9072\n",
    "Epoch: 7 | train_loss: 0.5376 | train_acc: 0.9219 | test_loss: 0.5253 | test_acc: 0.9176\n",
    "Epoch: 8 | train_loss: 0.5546 | train_acc: 0.7930 | test_loss: 0.5231 | test_acc: 0.9176\n",
    "Epoch: 9 | train_loss: 0.5638 | train_acc: 0.8008 | test_loss: 0.4919 | test_acc: 0.9280\n",
    "Epoch: 10 | train_loss: 0.5136 | train_acc: 0.8086 | test_loss: 0.4180 | test_acc: 0.8655\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3546e-b59c-4f86-8c99-32016d0aec67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1e8af-9b41-482e-9550-426c039081a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d076a-75e2-467b-9243-67478aef4cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d077d-379d-41ed-96e6-5499637f631a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d955b0-78ff-48de-b70c-de2af87f5c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d0357-596d-4596-8be1-3b891fb0bf74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1097d8-ac57-4585-93ac-199f1e78f801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b90d28-cd27-4607-a910-4086bbdbfc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b005d2-7845-4bc0-8f60-f21cab40701e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d000a6-b6ee-48c0-9f00-5ca33793d79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849ba6a2-7a37-488c-9afe-cfe02c73eaae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
